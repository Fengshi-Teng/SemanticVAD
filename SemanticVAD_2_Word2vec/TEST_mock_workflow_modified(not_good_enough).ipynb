{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e53a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "from speechbrain.inference.VAD import VAD\n",
    "import time\n",
    "import whisper\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "# Load models\n",
    "vad = VAD.from_hparams(source=\"speechbrain/vad-crdnn-libriparty\", savedir=\"tmp_vad_model\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "# Parameters\n",
    "SAMPLE_RATE = 16000\n",
    "CHUNK_DURATION = 0.1\n",
    "CHUNK_SIZE = int(SAMPLE_RATE * CHUNK_DURATION)\n",
    "BUFFER_SIZE = 5 * CHUNK_SIZE\n",
    "PRE_SPEECH_DURATION = 0.5\n",
    "PRE_SPEECH_SIZE = int(SAMPLE_RATE * PRE_SPEECH_DURATION)\n",
    "NUM_LABEL = 1\n",
    "\n",
    "# Buffers & states\n",
    "audio_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "pre_speech_buffer = deque(maxlen=PRE_SPEECH_SIZE)\n",
    "recorded_audio_buffer = []\n",
    "\n",
    "is_recording = False\n",
    "quiet_count = 0\n",
    "speech_count = 0\n",
    "recording_triggered = False\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"‚ö†Ô∏è\", status)\n",
    "    samples = indata[:, 0]\n",
    "    audio_buffer.extend(samples)\n",
    "    pre_speech_buffer.extend(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:,0]\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBERTClassifier(\n",
       "  (encoder): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoModel\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "encoder = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "test_model = \"models/SemanticVAD_2.pt\"\n",
    "Semantic_VAD_model = DistilBERTClassifier(base_model=encoder, num_labels=2)\n",
    "Semantic_VAD_model.load_state_dict(torch.load(test_model, map_location=\"mps\"))\n",
    "Semantic_VAD_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab294776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Listening... (press Ctrl+C to stop)\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  ÌïòÎ£®Í∞Ä ÎëêÎ†§ dari Î¶¨ÌóàÎäî\n",
      "‚è±Ô∏è Whisper took 1.020 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 1\n",
      "logits=tensor([[-1.9042,  1.8073]])\n",
      "Backchannel ignored!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.030 seconds\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  Hi\n",
      "‚è±Ô∏è Whisper took 0.284 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 1\n",
      "logits=tensor([[-2.9100,  2.8706]])\n",
      "Backchannel ignored!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.020 seconds\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  Excuse me?\n",
      "‚è±Ô∏è Whisper took 0.284 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 0\n",
      "logits=tensor([[ 5.3915, -5.1514]])\n",
      "Stop AI talking!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.019 seconds\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  I don't think so.\n",
      "‚è±Ô∏è Whisper took 0.293 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 1\n",
      "logits=tensor([[-2.9348,  2.8994]])\n",
      "Backchannel ignored!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.019 seconds\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  Well, you're right.\n",
      "‚è±Ô∏è Whisper took 0.277 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 1\n",
      "logits=tensor([[-2.9307,  2.8941]])\n",
      "Backchannel ignored!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.018 seconds\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  No.\n",
      "‚è±Ô∏è Whisper took 0.268 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 1\n",
      "logits=tensor([[-2.9126,  2.8719]])\n",
      "Backchannel ignored!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.017 seconds\n",
      "‚è∫Ô∏è Start recording...\n",
      "__________________________________________________\n",
      "üõë Stop recording. Running Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francisteng/anaconda3/envs/speechbrain-env/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Transcription:  listen to\n",
      "‚è±Ô∏è Whisper took 0.928 seconds\n",
      "__________________________________________________\n",
      "Semantic VAD result: 0\n",
      "logits=tensor([[ 5.2388, -5.0202]])\n",
      "Stop AI talking!!!!!!\n",
      "‚è±Ô∏è SemanticVAD took 0.020 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start mic\n",
    "stream = sd.InputStream(callback=audio_callback, channels=1, samplerate=SAMPLE_RATE)\n",
    "stream.start()\n",
    "print(\"üéôÔ∏è Listening... (press Ctrl+C to stop)\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if len(audio_buffer) >= CHUNK_SIZE:\n",
    "            chunk_np = np.array(list(audio_buffer)[-CHUNK_SIZE:])\n",
    "            chunk_tensor = torch.from_numpy(chunk_np).unsqueeze(0)\n",
    "            start = time.time()\n",
    "            speech_prob = vad.get_speech_prob_chunk(chunk_tensor).max().item()\n",
    "            end = time.time()\n",
    "\n",
    "            if speech_prob > 0.5:\n",
    "                speech_count += 1\n",
    "                quiet_count = 0\n",
    "                # print(f\"Speaking chunk {speech_count}/2 (prob={speech_prob:.2f}) | detection time consumption {end - start:.3f}s\")\n",
    "\n",
    "                if not is_recording and speech_count >= 2:\n",
    "                    print(\"‚è∫Ô∏è Start recording...\")\n",
    "                    is_recording = True\n",
    "                    recorded_audio_buffer = list(pre_speech_buffer) + list(chunk_np)\n",
    "                    recording_triggered = True\n",
    "                elif is_recording:\n",
    "                    recorded_audio_buffer.extend(chunk_np)\n",
    "\n",
    "            else:\n",
    "                if is_recording:\n",
    "                    recorded_audio_buffer.extend(chunk_np)\n",
    "                    quiet_count += 1\n",
    "                    # print(f\"Quiet chunk {quiet_count}/5\")\n",
    "\n",
    "                    if quiet_count >= 5:\n",
    "                        print(\"_\"*50)\n",
    "                        print(\"üõë Stop recording. Running Whisper...\")\n",
    "                        is_recording = False\n",
    "                        speech_count = 0\n",
    "                        quiet_count = 0\n",
    "                        recording_triggered = False\n",
    "\n",
    "                        \n",
    "                        # Save audio and transcribe\n",
    "                        a = time.time()\n",
    "                        audio_np = np.array(recorded_audio_buffer)\n",
    "                        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_wav:\n",
    "                            sf.write(tmp_wav.name, audio_np, SAMPLE_RATE)\n",
    "                            result = whisper_model.transcribe(tmp_wav.name)\n",
    "                            print(\"üìù Transcription:\", result[\"text\"])\n",
    "                        b = time.time()\n",
    "                        print(f\"‚è±Ô∏è Whisper took {b - a:.3f} seconds\")\n",
    "\n",
    "                        a = time.time()\n",
    "                        with torch.no_grad():\n",
    "                            inputs = tokenizer(\n",
    "                                result[\"text\"],\n",
    "                                truncation=True,\n",
    "                                padding=\"max_length\",\n",
    "                                max_length=128,\n",
    "                                return_tensors=\"pt\"  # make sure it returns tensor\n",
    "                            )\n",
    "                            # feed input_ids and attention_mask separately\n",
    "                            outputs = Semantic_VAD_model(\n",
    "                                input_ids=inputs[\"input_ids\"],\n",
    "                                attention_mask=inputs[\"attention_mask\"]\n",
    "                            )\n",
    "                            logits = outputs.logits\n",
    "                            # Softmax to get probability distribution\n",
    "                            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "                            # Get predicted class\n",
    "                            pred = torch.argmax(probs, dim=-1).item()\n",
    "                            print(\"_\"*50)\n",
    "                            print(f\"Semantic VAD result: {pred}\\n{logits=}\")\n",
    "\n",
    "                            if pred == 0:\n",
    "                                logiif_interrupt = 0\n",
    "                                print(\"Stop AI talking!!!!!!\")\n",
    "                            else:\n",
    "                                logiif_interrupt = 1\n",
    "                                print(\"Backchannel ignored!!!!!!\")\n",
    "                        b = time.time()\n",
    "                        print(f\"‚è±Ô∏è SemanticVAD took {b - a:.3f} seconds\")\n",
    "                else:\n",
    "                    speech_count = 0\n",
    "                    quiet_count = 0\n",
    "\n",
    "        sd.sleep(50)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"üîö Stop listening.\")\n",
    "    stream.stop()\n",
    "    stream.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechbrain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
